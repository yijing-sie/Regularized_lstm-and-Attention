{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY2E6awTopNp",
        "outputId": "60f4a92b-4169-4cfd-b757-1fc86f71b272"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iufzichcst1S",
        "outputId": "a3809317-6563-4f8c-ba78-e319f22c0049"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ED8LgzvQxnag"
      },
      "outputs": [],
      "source": [
        "!cp '/content/drive/MyDrive/Colab Notebooks/DL/hw4p1/hw4/tests.py' ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rOUiGUy-xrQT",
        "outputId": "7a43fe02-ee6b-48b9-d936-ad575fc74fae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-nlp\n",
            "  Downloading pytorch_nlp-0.5.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-nlp) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-nlp) (4.64.0)\n",
            "Installing collected packages: pytorch-nlp\n",
            "Successfully installed pytorch-nlp-0.5.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xeus-python\n",
            "  Downloading xeus_python-0.14.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 32.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments<3,>=2.3.1 in /usr/local/lib/python3.7/dist-packages (from xeus-python) (2.6.1)\n",
            "Collecting debugpy>=1.1.0\n",
            "  Downloading debugpy-1.6.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 58.1 MB/s \n",
            "\u001b[?25hCollecting xeus-python-shell<0.5,>=0.4.1\n",
            "  Downloading xeus_python_shell-0.4.2-py3-none-any.whl (7.6 kB)\n",
            "Collecting ipython<9,>=7.21\n",
            "  Downloading ipython-7.34.0-py3-none-any.whl (793 kB)\n",
            "\u001b[K     |████████████████████████████████| 793 kB 49.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython<9,>=7.21->xeus-python-shell<0.5,>=0.4.1->xeus-python) (57.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython<9,>=7.21->xeus-python-shell<0.5,>=0.4.1->xeus-python) (4.4.2)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython<9,>=7.21->xeus-python-shell<0.5,>=0.4.1->xeus-python) (0.1.3)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython<9,>=7.21->xeus-python-shell<0.5,>=0.4.1->xeus-python) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython<9,>=7.21->xeus-python-shell<0.5,>=0.4.1->xeus-python) (5.1.1)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.30-py3-none-any.whl (381 kB)\n",
            "\u001b[K     |████████████████████████████████| 381 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython<9,>=7.21->xeus-python-shell<0.5,>=0.4.1->xeus-python) (0.7.5)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython<9,>=7.21->xeus-python-shell<0.5,>=0.4.1->xeus-python) (4.8.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython<9,>=7.21->xeus-python-shell<0.5,>=0.4.1->xeus-python) (0.18.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython<9,>=7.21->xeus-python-shell<0.5,>=0.4.1->xeus-python) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython<9,>=7.21->xeus-python-shell<0.5,>=0.4.1->xeus-python) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<9,>=7.21->xeus-python-shell<0.5,>=0.4.1->xeus-python) (0.2.5)\n",
            "Installing collected packages: prompt-toolkit, ipython, debugpy, xeus-python-shell, xeus-python\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Attempting uninstall: debugpy\n",
            "    Found existing installation: debugpy 1.0.0\n",
            "    Uninstalling debugpy-1.0.0:\n",
            "      Successfully uninstalled debugpy-1.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.30 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.34.0 which is incompatible.\u001b[0m\n",
            "Successfully installed debugpy-1.6.2 ipython-7.34.0 prompt-toolkit-3.0.30 xeus-python-0.14.1 xeus-python-shell-0.4.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "debugpy",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install pytorch-nlp\n",
        "!pip install xeus-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODps5vcl1WEA",
        "outputId": "8deaeadc-ccd5-4a39-983d-363b430ea664"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/DL/hw4p1/dataset\n"
          ]
        }
      ],
      "source": [
        "%cd '/content/drive/MyDrive/Colab Notebooks/DL/hw4p1/dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxiZ42B4SwQ-",
        "outputId": "ac9ef2f1-fef8-4595-cbd3-bd9f657174e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tests import test_prediction, test_generation\n",
        "from torch.nn import Parameter\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using {} device'.format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "x5znxQhLSwRC"
      },
      "outputs": [],
      "source": [
        "# load all that we need\n",
        "dataset = np.load('../dataset/wiki.train.npy', allow_pickle=True) # training data\n",
        "fixtures_pred = np.load('../fixtures/prediction.npz')  # data for prediction() from the `dev` set\n",
        "fixtures_gen = np.load('../fixtures/generation.npy')  # input for generation() from the `dev` set\n",
        "fixtures_pred_test = np.load('../fixtures/prediction_test.npz') # data for prediction() from the test set\n",
        "fixtures_gen_test = np.load('../fixtures/generation_test.npy')  # input for generation() from the test set\n",
        "vocab = np.load('../dataset/vocab.npy') # a NumPy file containing the words in the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OZNrJ8XvSwRF"
      },
      "outputs": [],
      "source": [
        "#%%\n",
        "# data loader\n",
        "class LanguageModelDataLoader(DataLoader):\n",
        "    \"\"\"\n",
        "        TODO: Define data loader logic here\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, batch_size, shuffle=True):\n",
        "      self.dataset =  dataset\n",
        "      self.batch_size = batch_size\n",
        "      self.shuffle = shuffle\n",
        "\n",
        "    def __iter__(self):\n",
        "        if self.shuffle: np.random.shuffle(self.dataset)\n",
        "        # concatenate your articles and build into batches\n",
        "        #Concatenate all text in one long string.\n",
        "        con_dataset = np.hstack(dataset)\n",
        "        #a random BPTT length which is N(70, 5) with probability 0.95 and N(35, 5) with probability 0.05.\n",
        "        i = 0\n",
        "        while (1):\n",
        "            bptt_len = np.random.choice([int(np.random.normal(70, 5)), int(np.random.normal(35, 5))], p=[0.95, 0.05])\n",
        "            if i < (len(con_dataset) - (self.batch_size* bptt_len) -1):\n",
        "                batch_in = con_dataset[i: i+(self.batch_size* bptt_len)].reshape(self.batch_size, bptt_len)\n",
        "                target = con_dataset[i+1: i+1+(self.batch_size* bptt_len)].reshape(self.batch_size, bptt_len)\n",
        "                yield torch.from_numpy(batch_in).long().to(device), torch.from_numpy(target).long().to(device)\n",
        "                i += (self.batch_size* bptt_len)\n",
        "            else: \n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JIRpa_Iecl8z"
      },
      "outputs": [],
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim , dropout = 0.0, weight_init = 0.1):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim  = embedding_dim  \n",
        "        self.dropout = dropout\n",
        "        self.weight = nn.Parameter(torch.empty(vocab_size, embedding_dim, device=device))\n",
        "        #All embedding weights were uniformly initialized in the interval [−0.1, 0.1] \n",
        "        nn.init.uniform_(self.weight, -weight_init, weight_init)\n",
        "                     \n",
        "    def forward(self, x):\n",
        "        if self.training:\n",
        "          mask = torch.empty(self.vocab_size, 1, device = x.device).bernoulli_(1-self.dropout)/(1-self.dropout)\n",
        "          mask = mask.expand_as(self.weight)\n",
        "          weight = mask*self.weight\n",
        "          return weight[x]\n",
        "        else: return self.weight[x]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-rKc7BjfHXPX"
      },
      "outputs": [],
      "source": [
        "class LockedDropout(nn.Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "    def forward(self, x):\n",
        "        if not self.training or not self.p:\n",
        "            return x\n",
        "        mask = torch.empty(*x.shape[-2:], requires_grad=False, device = x.device).bernoulli_(1 - self.p)/(1 - self.p)\n",
        "        mask = mask.expand_as(x)\n",
        "        return mask * x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "fvb6o5VgbGW5"
      },
      "outputs": [],
      "source": [
        "class Weight_Tying(nn.Module):\n",
        "    def __init__(self, embed_size, out_size, embed_weight):\n",
        "        super().__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.out_size = out_size\n",
        "        self.embed_weight = embed_weight\n",
        "        self.module = nn.Linear(self.embed_size, self.out_size, device=device)\n",
        "        b_init = 1.0/np.sqrt(self.out_size)\n",
        "        nn.init.uniform_(self.module._parameters['bias'], -b_init, b_init)\n",
        "        self.module._parameters['weight'] = embed_weight\n",
        "    def forward(self, *args, **kwargs):\n",
        "        return self.module.forward(*args, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "cEdBKV-2bGW5"
      },
      "outputs": [],
      "source": [
        "class LanguageModel(nn.Module):\n",
        "    \"\"\"\n",
        "        TODO: Define your model here\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        #embedding dropout \n",
        "        self.embedding = Embedding(vocab_size=vocab_size, embedding_dim=400, dropout = 0.1)\n",
        "        self.embedding_dp = LockedDropout(p=0.4)\n",
        "        self.out_dp = LockedDropout(p=0.4)\n",
        "        #three-layer LSTM model with 1150 units in the hidden layer and an embedding of size 400.\n",
        "        self.lstm = nn.LSTM(input_size=400, hidden_size=1150, num_layers=3, batch_first = True, device=device, proj_size=400, dropout=0.5)\n",
        "        #Weight tying (Inan et al., 2016; Press &Wolf, 2016) shares the weights between the embedding and softmax layer\n",
        "        self.out = Weight_Tying(400, vocab_size, self.embedding.weight) \n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        x = self.embedding_dp(x)\n",
        "        x, h = self.lstm(x, hidden) if hidden else self.lstm(x)\n",
        "        x = self.out_dp(x)\n",
        "        hidden= h\n",
        "        out = self.out(x)\n",
        "        return out, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "kIvZOIfjSwRK"
      },
      "outputs": [],
      "source": [
        "# model trainer\n",
        "\n",
        "class LanguageModelTrainer:\n",
        "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
        "        \"\"\"\n",
        "            Use this class to train your model\n",
        "        \"\"\"\n",
        "        # feel free to add any other parameters here\n",
        "        self.model = model\n",
        "        self.loader = loader\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.predictions = []\n",
        "        self.predictions_test = []\n",
        "        self.generated_logits = []\n",
        "        self.generated = []\n",
        "        self.generated_logits_test = []\n",
        "        self.generated_test = []\n",
        "        self.epochs = 0\n",
        "        self.max_epochs = max_epochs\n",
        "        self.run_id = run_id\n",
        "        \n",
        "        # TODO: Define your optimizer and criterion here\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001, weight_decay=1e-6)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "    def train(self):\n",
        "        self.model.train() # set to training mode\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "        for batch_num, (inputs, targets) in enumerate(self.loader):\n",
        "            epoch_loss += self.train_batch(inputs, targets)\n",
        "        epoch_loss = epoch_loss / (batch_num + 1)\n",
        "        self.epochs += 1\n",
        "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, epoch_loss))\n",
        "        self.train_losses.append(epoch_loss)\n",
        "\n",
        "    def train_batch(self, inputs, targets):\n",
        "        \"\"\" \n",
        "            TODO: Define code for training a single batch of inputs\n",
        "        \n",
        "        \"\"\"\n",
        "        self.optimizer.zero_grad()\n",
        "        predict,_ = self.model(inputs) \n",
        "        loss = self.criterion(predict.view(-1, predict.shape[2]), targets.view(-1)) \n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    \n",
        "    def test(self):\n",
        "        # don't change these\n",
        "        self.model.eval() # set to eval mode\n",
        "        predictions = TestLanguageModel.prediction(fixtures_pred['inp'], self.model) # get predictions for `dev`\n",
        "        self.predictions.append(predictions)\n",
        "        generated_logits = TestLanguageModel.generation(fixtures_gen, 10, self.model) # generated predictions for 10 words for `dev`\n",
        "        generated_logits_test = TestLanguageModel.generation(fixtures_gen_test, 10, self.model)  # generated predictions for 10 words for `test`\n",
        "        nll = test_prediction(predictions, fixtures_pred['out']) # Negative Log Likelihood (NLL) on the dev set for prediction\n",
        "        generated = test_generation(fixtures_gen, generated_logits, vocab) # generated sequence for `dev`\n",
        "        generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab) # generated sequence for `test`\n",
        "        self.val_losses.append(nll)\n",
        "        \n",
        "        self.generated.append(generated)\n",
        "        self.generated_test.append(generated_test)\n",
        "        self.generated_logits.append(generated_logits)\n",
        "        self.generated_logits_test.append(generated_logits_test)\n",
        "        \n",
        "        # generate predictions for test data\n",
        "        predictions_test = TestLanguageModel.prediction(fixtures_pred_test['inp'], self.model) # get predictions for `test`\n",
        "        self.predictions_test.append(predictions_test)\n",
        "            \n",
        "        print('[VAL]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, nll))\n",
        "        return nll\n",
        "\n",
        "    def save(self):\n",
        "        # don't change these\n",
        "        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
        "        torch.save({'state_dict': self.model.state_dict()},\n",
        "            model_path)\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
        "        with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated[-1])\n",
        "        with open(os.path.join('experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated_test[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "xPI7_kZRSwRN"
      },
      "outputs": [],
      "source": [
        "class TestLanguageModel:\n",
        "    def prediction(inp, model):\n",
        "        \"\"\"\n",
        "            TODO: write prediction code here\n",
        "            \n",
        "            :param inp:\n",
        "            :return: a np.ndarray of logits\n",
        "        \"\"\"  \n",
        "        inp = torch.from_numpy(inp).long().to(device)\n",
        "        out,_ = model(inp)\n",
        "        return out[:,-1,:].cpu().detach().numpy()\n",
        "\n",
        "        \n",
        "    def generation(inp, forward, model):\n",
        "        \"\"\"\n",
        "            TODO: write generation code here\n",
        "\n",
        "            Generate a sequence of words given a starting sequence.\n",
        "            :param inp: Initial sequence of words (batch size, length)\n",
        "            :param forward: number of additional words to generate\n",
        "            :return: generated words (batch size, forward)\n",
        "        \"\"\"   \n",
        "        model.eval()\n",
        "        inp = torch.from_numpy(inp).long().to(device)\n",
        "        with torch.no_grad():\n",
        "            h = None\n",
        "            generated_words = []\n",
        "            for i in range(forward):                           \n",
        "                out, h = model(inp, h)\n",
        "                out = out[:,-1,:].argmax(1)\n",
        "                inp = torch.cat((inp, out.unsqueeze(1)), 1)\n",
        "                generated_words.append(out.cpu().detach().numpy())\n",
        "        generated_words = np.stack(generated_words, 1)\n",
        "        return generated_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "TiUrjbEjSwRQ"
      },
      "outputs": [],
      "source": [
        "# TODO: define other hyperparameters here\n",
        "\n",
        "NUM_EPOCHS = 50\n",
        "BATCH_SIZE = 80"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HCVG5YISwRW",
        "outputId": "26d13346-18bb-4ddd-f37f-5304be47f31f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving models, predictions, and generated words to ./experiments/1660560996\n"
          ]
        }
      ],
      "source": [
        "run_id = str(int(time.time()))\n",
        "if not os.path.exists('./experiments'):\n",
        "    os.mkdir('./experiments')\n",
        "os.mkdir('./experiments/%s' % run_id)\n",
        "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "DbHH6zXTSwRa"
      },
      "outputs": [],
      "source": [
        "model = LanguageModel(len(vocab)).to(device)\n",
        "loader = LanguageModelDataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "trainer = LanguageModelTrainer(model=model, loader=loader, max_epochs=NUM_EPOCHS, run_id=run_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7D8wTJkBSwRc",
        "outputId": "87c6d7f7-111f-4e3f-b1f5-1a8162a2ed11"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [2/50]   Loss: 7.1620\n",
            "[VAL]  Epoch [2/50]   Loss: 5.8730\n",
            "Saving model, predictions and generated output for epoch 0 with NLL: 5.8730087\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|▏         | 1/50 [02:14<1:50:00, 134.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [3/50]   Loss: 6.4461\n",
            "[VAL]  Epoch [3/50]   Loss: 5.5622\n",
            "Saving model, predictions and generated output for epoch 1 with NLL: 5.562196\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|▍         | 2/50 [04:38<1:52:00, 140.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [4/50]   Loss: 6.1550\n",
            "[VAL]  Epoch [4/50]   Loss: 5.2079\n",
            "Saving model, predictions and generated output for epoch 2 with NLL: 5.2079177\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  6%|▌         | 3/50 [07:06<1:52:28, 143.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [5/50]   Loss: 5.9416\n",
            "[VAL]  Epoch [5/50]   Loss: 4.9592\n",
            "Saving model, predictions and generated output for epoch 3 with NLL: 4.959221\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  8%|▊         | 4/50 [09:35<1:51:54, 145.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [6/50]   Loss: 5.7682\n",
            "[VAL]  Epoch [6/50]   Loss: 4.8162\n",
            "Saving model, predictions and generated output for epoch 4 with NLL: 4.8162184\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 5/50 [12:05<1:50:26, 147.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [7/50]   Loss: 5.6331\n",
            "[VAL]  Epoch [7/50]   Loss: 4.6529\n",
            "Saving model, predictions and generated output for epoch 5 with NLL: 4.6529293\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 12%|█▏        | 6/50 [14:35<1:48:47, 148.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [8/50]   Loss: 5.5093\n",
            "[VAL]  Epoch [8/50]   Loss: 4.6012\n",
            "Saving model, predictions and generated output for epoch 6 with NLL: 4.601205\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 14%|█▍        | 7/50 [17:05<1:46:43, 148.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [9/50]   Loss: 5.4032\n",
            "[VAL]  Epoch [9/50]   Loss: 4.5504\n",
            "Saving model, predictions and generated output for epoch 7 with NLL: 4.5504246\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 16%|█▌        | 8/50 [19:36<1:44:30, 149.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [10/50]   Loss: 5.3185\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 18%|█▊        | 9/50 [22:05<1:42:07, 149.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL]  Epoch [10/50]   Loss: 4.5866\n",
            "[TRAIN]  Epoch [11/50]   Loss: 5.2348\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 10/50 [24:35<1:39:42, 149.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL]  Epoch [11/50]   Loss: 4.5563\n",
            "[TRAIN]  Epoch [12/50]   Loss: 5.1660\n",
            "[VAL]  Epoch [12/50]   Loss: 4.5393\n",
            "Saving model, predictions and generated output for epoch 10 with NLL: 4.539317\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 22%|██▏       | 11/50 [27:05<1:37:19, 149.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [13/50]   Loss: 5.1044\n",
            "[VAL]  Epoch [13/50]   Loss: 4.5012\n",
            "Saving model, predictions and generated output for epoch 11 with NLL: 4.501224\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 24%|██▍       | 12/50 [29:36<1:34:57, 149.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [14/50]   Loss: 5.0543\n",
            "[VAL]  Epoch [14/50]   Loss: 4.4277\n",
            "Saving model, predictions and generated output for epoch 12 with NLL: 4.4277406\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 26%|██▌       | 13/50 [32:07<1:32:39, 150.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [15/50]   Loss: 4.9892\n",
            "[VAL]  Epoch [15/50]   Loss: 4.3846\n",
            "Saving model, predictions and generated output for epoch 13 with NLL: 4.38455\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 28%|██▊       | 14/50 [34:38<1:30:16, 150.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [16/50]   Loss: 4.9381\n",
            "[VAL]  Epoch [16/50]   Loss: 4.3341\n",
            "Saving model, predictions and generated output for epoch 14 with NLL: 4.334116\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 15/50 [37:09<1:27:58, 150.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [17/50]   Loss: 4.8938\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 32%|███▏      | 16/50 [39:40<1:25:27, 150.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL]  Epoch [17/50]   Loss: 4.3680\n",
            "[TRAIN]  Epoch [18/50]   Loss: 4.8532\n",
            "[VAL]  Epoch [18/50]   Loss: 4.2844\n",
            "Saving model, predictions and generated output for epoch 16 with NLL: 4.284438\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 34%|███▍      | 17/50 [42:11<1:23:00, 150.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [19/50]   Loss: 4.8147\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 36%|███▌      | 18/50 [44:42<1:20:28, 150.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL]  Epoch [19/50]   Loss: 4.3977\n",
            "[TRAIN]  Epoch [20/50]   Loss: 4.7773\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 38%|███▊      | 19/50 [47:12<1:17:52, 150.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL]  Epoch [20/50]   Loss: 4.3243\n",
            "[TRAIN]  Epoch [21/50]   Loss: 4.7396\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 20/50 [49:43<1:15:21, 150.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL]  Epoch [21/50]   Loss: 4.3913\n",
            "[TRAIN]  Epoch [22/50]   Loss: 4.7158\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 42%|████▏     | 21/50 [52:14<1:12:50, 150.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL]  Epoch [22/50]   Loss: 4.3000\n",
            "Epoch 00021: reducing learning rate of group 0 to 5.0000e-04.\n",
            "[TRAIN]  Epoch [23/50]   Loss: 4.5789\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 44%|████▍     | 22/50 [54:44<1:10:17, 150.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL]  Epoch [23/50]   Loss: 4.2999\n",
            "[TRAIN]  Epoch [24/50]   Loss: 4.5420\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 46%|████▌     | 23/50 [57:15<1:07:46, 150.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL]  Epoch [24/50]   Loss: 4.3002\n",
            "[TRAIN]  Epoch [25/50]   Loss: 4.5040\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 48%|████▊     | 24/50 [59:45<1:05:14, 150.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL]  Epoch [25/50]   Loss: 4.2918\n",
            "[TRAIN]  Epoch [26/50]   Loss: 4.4820\n",
            "[VAL]  Epoch [26/50]   Loss: 4.2511\n",
            "Saving model, predictions and generated output for epoch 24 with NLL: 4.2510943\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 25/50 [1:02:16<1:02:47, 150.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [27/50]   Loss: 4.4718\n",
            "[VAL]  Epoch [27/50]   Loss: 4.2464\n",
            "Saving model, predictions and generated output for epoch 25 with NLL: 4.2463894\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 52%|█████▏    | 26/50 [1:04:47<1:00:16, 150.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [28/50]   Loss: 4.4530\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 54%|█████▍    | 27/50 [1:07:17<57:42, 150.56s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL]  Epoch [28/50]   Loss: 4.2610\n",
            "[TRAIN]  Epoch [29/50]   Loss: 4.4255\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 56%|█████▌    | 28/50 [1:09:48<55:12, 150.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL]  Epoch [29/50]   Loss: 4.3241\n",
            "[TRAIN]  Epoch [30/50]   Loss: 4.4173\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 58%|█████▊    | 29/50 [1:12:19<52:45, 150.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL]  Epoch [30/50]   Loss: 4.2836\n",
            "[TRAIN]  Epoch [31/50]   Loss: 4.4073\n",
            "[VAL]  Epoch [31/50]   Loss: 4.2425\n",
            "Saving model, predictions and generated output for epoch 29 with NLL: 4.242475\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 30/50 [1:14:51<50:20, 151.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [32/50]   Loss: 4.3929\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 62%|██████▏   | 31/50 [1:17:21<47:47, 150.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL]  Epoch [32/50]   Loss: 4.2555\n",
            "[TRAIN]  Epoch [33/50]   Loss: 4.3775\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 64%|██████▍   | 32/50 [1:19:52<45:17, 150.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL]  Epoch [33/50]   Loss: 4.2559\n",
            "[TRAIN]  Epoch [34/50]   Loss: 4.3594\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 66%|██████▌   | 33/50 [1:22:23<42:45, 150.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL]  Epoch [34/50]   Loss: 4.2696\n",
            "[TRAIN]  Epoch [35/50]   Loss: 4.3377\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 68%|██████▊   | 34/50 [1:24:54<40:15, 150.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL]  Epoch [35/50]   Loss: 4.2802\n",
            "Epoch 00034: reducing learning rate of group 0 to 2.5000e-04.\n",
            "[TRAIN]  Epoch [36/50]   Loss: 4.2623\n",
            "[VAL]  Epoch [36/50]   Loss: 4.2295\n",
            "Saving model, predictions and generated output for epoch 34 with NLL: 4.229457\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 35/50 [1:27:26<37:45, 151.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [37/50]   Loss: 4.2435\n",
            "[VAL]  Epoch [37/50]   Loss: 4.2107\n",
            "Saving model, predictions and generated output for epoch 35 with NLL: 4.210659\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 72%|███████▏  | 36/50 [1:29:57<35:17, 151.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [38/50]   Loss: 4.2413\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 74%|███████▍  | 37/50 [1:32:28<32:44, 151.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL]  Epoch [38/50]   Loss: 4.2216\n",
            "[TRAIN]  Epoch [39/50]   Loss: 4.2271\n",
            "[VAL]  Epoch [39/50]   Loss: 4.2072\n",
            "Saving model, predictions and generated output for epoch 37 with NLL: 4.2071533\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 76%|███████▌  | 38/50 [1:35:00<30:16, 151.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [40/50]   Loss: 4.2211\n",
            "[VAL]  Epoch [40/50]   Loss: 4.2003\n",
            "Saving model, predictions and generated output for epoch 38 with NLL: 4.200268\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 78%|███████▊  | 39/50 [1:39:39<28:06, 153.33s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-b25bd27e8b58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbest_nll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mnll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-3f49f0655cc0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_num\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-3f49f0655cc0>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \"\"\"\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-ead9e93404c6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_dp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mhidden\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 770\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    771\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "start=0\n",
        "best_nll = 1e30 \n",
        "for epoch in tqdm(range(start, start+NUM_EPOCHS)):\n",
        "    trainer.train()\n",
        "    nll = trainer.test()\n",
        "    trainer.scheduler.step(nll)\n",
        "    if nll < best_nll:\n",
        "        best_nll = nll\n",
        "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))\n",
        "        trainer.save() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "z2FmDqBCSwRf",
        "outputId": "99867479-39c3-42a2-dfc0-27451016dac7"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot training curves\n",
        "plt.figure()\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses on prediction of a single word')\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation losses on prediction of a single word')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('NLL')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('loss.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipdbmqaGSwRh",
        "outputId": "ba8b7293-d4d0-4456-d098-5cdea98fed23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input | Output #0: while the group was en route , but only three were ultimately able to attack . None of them were | also killed , and the men were forced to retreat\n",
            "Input | Output #1: <unk> , where he remained on loan until 30 June 2010 . <eol> = = = Return to Manchester United | = = = <eol> Torres was appointed manager of the\n",
            "Input | Output #2: 25 April 2013 , denoting shipments of 500 @,@ 000 copies . <eol> The song became One Direction 's fourth | album , and the second single from the album ,\n",
            "Input | Output #3: , and Bruce R. ) one daughter ( Wendy J. <unk> ) and two grandchildren , died in <unk> , | <unk> , and <unk> . <eol> = = = <unk>\n",
            "Input | Output #4: Warrior were examples of this type . Because their armor was so heavy , they could only carry a single | screw <unk> and could be used to reduce the efficiency\n",
            "Input | Output #5: the embassy at 1 : 49 and landed on Guam at 2 : 23 ; twenty minutes later , Ambassador | <unk> <unk> <unk> , the commander of the <unk> <unk>\n",
            "Input | Output #6: <unk> , $ 96 million USD ) . Damage was heaviest in South Korea , notably where it moved ashore | . The storm was estimated to be $ 1 @.@\n",
            "Input | Output #7: The <unk> were condemned as <unk> by <unk> , who saw the riots as hampering attempts to resolve the situation | . <eol> = = = <unk> = = = <eol>\n",
            "Input | Output #8: by a decision made by the War Office in mid @-@ 1941 , as it was considering the equipment to | be completed . <eol> = = = <unk> = =\n",
            "Input | Output #9: Division crossed the <unk> at a number of places and climbed the hills quietly toward the 9th Infantry river line | . The 766th Regiment was sent to the west of\n",
            "Input | Output #10: = <eol> = = = French VIII . Corps ( Corps <unk> ) = = = <eol> On 6 November | , the 2nd Battalion , 9th Marines , was ordered\n",
            "Input | Output #11: of the World from 9th Avenue \" . This is regarded as his most famous work . It is considered | a \" <unk> \" , and the \" <unk> \"\n",
            "Input | Output #12: — <unk> @-@ 10 , <unk> @-@ 12 , <unk> @-@ 16 , <unk> @-@ 17 — were all converted | to the <unk> . <eol> = = = <unk> =\n",
            "Input | Output #13: And now he has . \" <eol> = = Family = = <eol> <unk> lived 37 of his years in | the first half of the year . His father ,\n",
            "Input | Output #14: Hell to which he has been condemned for <unk> . Eliot , in a letter to John <unk> dated 27 | February 1820 , was a <unk> of the <unk> <unk>\n",
            "Input | Output #15: Luoyang area , fulfilling his duties in domestic affairs . <eol> In the autumn of <unk> , he met Li | <unk> , a former immigrant who had been born in\n",
            "Input | Output #16: Power said they enjoyed Block Ball and its number of stages , but wondered how its eight <unk> of memory | were not a violation of the game . <eol> =\n",
            "Input | Output #17: by Lloyd F. Lonergan . The cameraman was Jacques <unk> . <eol> = = Release and reception = = <eol> | The film was directed by <unk> Bramantyo , who was\n",
            "Input | Output #18: alone , the Austrians lost more than half their reserve artillery park , 6 @,@ 000 ( out of 8 | @,@ 000 ) . The 766th Regiment was sent to\n",
            "Input | Output #19: while attacking a ship at <unk> in the Dutch East Indies ; the loss was compounded by the fact that | the British fleet had been unable to seize the British\n",
            "Input | Output #20: first raised in 2007 by the member of parliament ( MP ) for <unk> . The gangsters may have run | in the city , and the city is not a\n",
            "Input | Output #21: Species are also non @-@ spiny <unk> and includes both large trees with stout stems up to 30 metres ( | 66 – 40 ft ) . The fruit bodies are\n",
            "Input | Output #22: \" : specific design issues with the building 's energy efficiency included the fact that the largest room in the | tower was to be completed . <eol> = = =\n",
            "Input | Output #23: were reported to support over 300 @,@ 000 households in the Brazilian state of <unk> in 2005 , and in | 2010 the company was granted a new share of $\n",
            "Input | Output #24: port . <unk> in Vietnam also warned for the potential of heavy rainfall due to the dissipating Tropical Depression <unk> | @-@ E . <eol> = = = Tropical Storm Brenda\n",
            "Input | Output #25: T @-@ numbers in their tropical cyclone products . The following example is from discussion number 3 of Tropical Depression | Six @-@ E , which is a tropical storm that\n",
            "Input | Output #26: South Australia hosted the three @-@ game semi @-@ final series against the New South Wales <unk> . Both teams | were also the first to win the Grand Slam in\n",
            "Input | Output #27: Perth from contention and secured the last finals spot for the <unk> . <eol> = = = Statistical leaders = | = = <eol> The first leg of the season was\n",
            "Input | Output #28: deemed it an \" amazing pop song \" , lauding the group 's falsetto and its \" head @-@ <unk> | \" . <eol> = = = <unk> = = =\n",
            "Input | Output #29: , but began patrolling the English Channel after <unk> @-@ 6 pioneered a route past British <unk> nets and mines | . <eol> = = = <unk> = = = <eol>\n",
            "Input | Output #30: production executives to let him direct . He had already discussed the film with <unk> and Cohen , and felt | that he had been a \" <unk> \" . He\n",
            "Input | Output #31: and Nick <unk> at Studio <unk> in Los Angeles , California , and was released on August 1 , 2006 | . The album was released on November 28 , 2003\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# see generated output\n",
        "print (trainer.generated[-1]) # get last generated output"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "training.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
